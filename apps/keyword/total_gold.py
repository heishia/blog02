"""
í™©ê¸ˆí‚¤ì›Œë“œ í†µí•© ë„êµ¬ (total_gold.py)
- ì—°ê´€í‚¤ì›Œë“œ í™•ì¥ + ê²€ìƒ‰ëŸ‰/ë¬¸ì„œëŸ‰ ë¶„ì„ + í•„í„°ë§ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ í†µí•©
- related.py + gold.py ê¸°ëŠ¥ì„ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰
"""

import hashlib
import hmac
import time
import requests
import urllib.request
import urllib.parse
import json
from typing import List, Dict, Set, Tuple
import yaml
import pandas as pd
from datetime import datetime
import os
from collections import deque
from pathlib import Path
import random
from threading import Thread
import sys
import logging

# ================================================================================
# ğŸ”§ ì„¤ì • ìƒìˆ˜ê°’ë“¤ (ì—¬ê¸°ì„œ ëª¨ë“  ì„¤ì • ê´€ë¦¬)
# ================================================================================

# ğŸ“Œ ë¡œê¹… ì„¤ì •
def setup_logging():
    """ë¡œê¹… ì„¤ì • - í„°ë¯¸ë„ê³¼ íŒŒì¼ì— ë™ì‹œ ì¶œë ¥"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # ë¡œê·¸ íŒŒì¼ëª… (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"{log_dir}/total_gold_{timestamp}.log"
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),  # íŒŒì¼ì— ì €ì¥
            logging.StreamHandler(sys.stdout)  # í„°ë¯¸ë„ì— ì¶œë ¥
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¡œê·¸ íŒŒì¼ ìƒì„±: {log_file}")
    return logger

# ì „ì—­ ë¡œê±° ì„¤ì •
logger = setup_logging()

# print í•¨ìˆ˜ë¥¼ loggerë¡œ ëŒ€ì²´í•˜ëŠ” í•¨ìˆ˜
def log_print(message):
    """print ëŒ€ì‹  ì‚¬ìš©í•  ë¡œê¹… í•¨ìˆ˜"""
    logger.info(message)
    
# ğŸ“Œ í‚¤ì›Œë“œ ì£¼ì œ ì„¤ì •
KEYWORD_SUBJECT = "ê²Œì„"
CUSTOM_SEED_KEYWORDS = ["ê²Œì„"]  # ì§ì ‘ ì§€ì •í•  ì‹œë“œí‚¤ì›Œë“œ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ JSON íŒŒì¼ì—ì„œ ë¡œë“œ)
# ì˜ˆì‹œ: ["ê²Œì„"], ["SNS"], ["ì‡¼í•‘"], ["ì—¬í–‰"] ë“± ììœ ë¡­ê²Œ ë³€ê²½ ê°€ëŠ¥

# ğŸ“Œ ì—°ê´€í‚¤ì›Œë“œ í™•ì¥ ì„¤ì •
MAX_KEYWORDS = 1000  # ìµœëŒ€ ì—°ê´€í‚¤ì›Œë“œ ìˆ˜ (API ì œí•œ ê³ ë ¤í•˜ì—¬ 200ê°œë¡œ ì¡°ì •)
MAX_SEED_KEYWORDS = 1  # ì‚¬ìš©í•  ì‹œë“œí‚¤ì›Œë“œ 1ê°œë¡œ ì œí•œ
DELAY_BETWEEN_REQUESTS = 0.05  # API ìš”ì²­ ê°„ ê°„ê²© (ì´ˆ) - ì´ˆê³ ì† ì²˜ë¦¬

# ğŸ“Œ í™©ê¸ˆí‚¤ì›Œë“œ ë¶„ì„ ì„¤ì •
MODE = "BASIC"  # "BASIC" (ê¸°ë³¸ëª¨ë“œ), "AUTO" (ìë™ëª¨ë“œ), "BACKGROUND" (ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ)
TARGET_STAGES = [1, 2, 3, 4, 5]  # ëª©í‘œë¡œ í•˜ëŠ” ìŠ¤í…Œì´ì§€ ì´ë‚´ (1-3ë‹¨ê³„ ì´ë‚´)

# ğŸ“Œ ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ ì„¤ì •
AUTO_CYCLE_MINUTES = 30  # ìë™ëª¨ë“œ ì‹¤í–‰ ì£¼ê¸° (ë¶„)
AUTO_RANDOM_SUBJECTS = ["ê²Œì„", "SNS"]  # ëœë¤ ì„ íƒí•  ì£¼ì œ ëª©ë¡ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ëª¨ë“  available ì£¼ì œ)

# ğŸ“Œ ë„¤ì´ë²„ ìë™ì™„ì„± API ì„¤ì •
NAVER_AC_URL = "https://mac.search.naver.com/mobile/ac"
NAVER_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 10; Pixel 3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0 Mobile Safari/537.36",
    "Referer": "https://m.search.naver.com/"
}

# ================================================================================
# ğŸ” ì—°ê´€í‚¤ì›Œë“œ í™•ì¥ ë¡œì§ (related.py ê¸°ë°˜)
# ================================================================================

def get_naver_autocomplete(keyword: str) -> List[str]:
    """ë„¤ì´ë²„ ìë™ì™„ì„± API í˜¸ì¶œí•˜ì—¬ ì—°ê´€í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        params = {
            "q": keyword,
            "st": 1,
            "frm": "mobile_nv", 
            "r_format": "json",
            "r_enc": "UTF-8",
            "r_unicode": 0,
            "r_lt": "koreng",
            "enc": "UTF-8",
            "ans": 1,
            "run": 2,
            "rev": 4,
            "callback": "jsonp12345"
        }
        
        response = requests.get(NAVER_AC_URL, params=params, headers=NAVER_HEADERS, timeout=10)
        response.raise_for_status()
        
        # ì‘ë‹µ ì²˜ë¦¬
        text = response.text
        
        # JSONP ë˜ëŠ” JSON í˜•íƒœ ì²˜ë¦¬
        data = None
        if text.startswith("jsonp12345(") and text.endswith(");"):
            # JSONP í˜•íƒœ
            json_str = text[11:-2]
            data = json.loads(json_str)
        else:
            # ìˆœìˆ˜ JSON í˜•íƒœ
            data = json.loads(text)
        
        # ìë™ì™„ì„± ê²°ê³¼ ì¶”ì¶œ
        suggestions = []
        
        # items ë°°ì—´ì—ì„œ ì¶”ì¶œ
        if "items" in data and len(data["items"]) > 0:
            for item in data["items"]:
                if isinstance(item, list) and len(item) > 0:
                    for sub_item in item:
                        if isinstance(sub_item, list) and len(sub_item) > 0:
                            suggestion = sub_item[0]
                            if suggestion and suggestion != keyword and suggestion not in suggestions:
                                suggestions.append(suggestion)
        
        return suggestions  # ëª¨ë“  ì—°ê´€í‚¤ì›Œë“œ ë°˜í™˜ (ì œí•œ ì—†ìŒ)
            
    except Exception as e:
        print(f"âŒ {keyword} API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def expand_keywords_recursive(seed_keywords: List[str], max_keywords: int = MAX_KEYWORDS) -> List[str]:
    """ì¬ê·€ì ìœ¼ë¡œ í‚¤ì›Œë“œ í™•ì¥í•˜ì—¬ ì—°ê´€í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
    
    # ê²°ê³¼ ì €ì¥
    processed_keywords = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ í‚¤ì›Œë“œë“¤
    all_keywords = set(seed_keywords)  # ì „ì²´ í‚¤ì›Œë“œ í’€
    
    # íë¥¼ ì‚¬ìš©í•œ BFS ë°©ì‹ìœ¼ë¡œ í™•ì¥ (ì›ë³¸ related.py ë¡œì§)
    queue = deque()
    
    # ì‹œë“œí‚¤ì›Œë“œë“¤ì„ íì— ì¶”ê°€ (í‚¤ì›Œë“œ, ì›ë³¸ì‹œë“œ) íŠœí”Œ í˜•íƒœë¡œ
    for seed in seed_keywords:
        queue.append((seed, seed))
    
    # ì§„í–‰ë¥  í‘œì‹œìš©
    total_processed = 0
    
    consecutive_empty_results = 0  # ì—°ì†ìœ¼ë¡œ ë¹ˆ ê²°ê³¼ê°€ ë‚˜ì˜¨ íšŸìˆ˜
    max_empty_tolerance = 50  # ì—°ì† 50ë²ˆ ë¹ˆ ê²°ê³¼ê¹Œì§€ í—ˆìš©
    
    while queue and len(all_keywords) < max_keywords:
        current_keyword, original_seed = queue.popleft()
        
        # ì´ë¯¸ ì²˜ë¦¬í•œ í‚¤ì›Œë“œëŠ” ê±´ë„ˆë›°ê¸°
        if current_keyword in processed_keywords:
            continue
        
        processed_keywords.add(current_keyword)
        total_processed += 1
        
        # ë„¤ì´ë²„ ìë™ì™„ì„± API í˜¸ì¶œ
        related_keywords = get_naver_autocomplete(current_keyword)
        
        if related_keywords:
            consecutive_empty_results = 0  # ì„±ê³µí•˜ë©´ ë¦¬ì…‹
            
            # ìƒˆë¡œìš´ í‚¤ì›Œë“œë“¤ë§Œ ì¶”ê°€
            new_keywords = []
            for kw in related_keywords:
                if kw not in all_keywords:
                    all_keywords.add(kw)
                    new_keywords.append(kw)
                    
                    # ë‹¤ìŒ í™•ì¥ì„ ìœ„í•´ íì— ì¶”ê°€ (í‚¤ì›Œë“œ, ì›ë³¸ì‹œë“œ) íŠœí”Œë¡œ
                    if len(all_keywords) < max_keywords:
                        queue.append((kw, original_seed))
        else:
            consecutive_empty_results += 1
            # ë„ˆë¬´ ë§ì€ ë¹ˆ ê²°ê³¼ê°€ ì—°ì†ìœ¼ë¡œ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨ (API ë¬¸ì œì¼ ê°€ëŠ¥ì„±)
            if consecutive_empty_results >= max_empty_tolerance:
                print(f"âš ï¸ ì—°ì† {max_empty_tolerance}ë²ˆ ë¹ˆ ê²°ê³¼ - í™•ì¥ ì¤‘ë‹¨")
                break
        
        # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
        if queue:  # ë§ˆì§€ë§‰ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë”œë ˆì´
            try:
                time.sleep(DELAY_BETWEEN_REQUESTS)
            except KeyboardInterrupt:
                break
        
        # ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
        if len(all_keywords) >= max_keywords:
            break
    
    return list(all_keywords)


def load_seed_keywords_from_json(keyword_subject: str) -> List[str]:
    """ì‹œë“œí‚¤ì›Œë“œ ë¡œë“œ (ìƒìˆ˜ì—ì„œ ìš°ì„ , ì—†ìœ¼ë©´ JSON íŒŒì¼ì—ì„œ)"""
    
    # 1. ìƒìˆ˜ë¡œ ì§ì ‘ ì§€ì •ëœ ì‹œë“œí‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if CUSTOM_SEED_KEYWORDS:
        print(f"ğŸ”§ ìƒìˆ˜ì—ì„œ ì‹œë“œí‚¤ì›Œë“œ ë¡œë“œ: {CUSTOM_SEED_KEYWORDS}")
        # ì‹œë“œí‚¤ì›Œë“œ ê°œìˆ˜ ì œí•œ
        if len(CUSTOM_SEED_KEYWORDS) > MAX_SEED_KEYWORDS:
            limited_keywords = CUSTOM_SEED_KEYWORDS[:MAX_SEED_KEYWORDS]
            print(f"âš ï¸ ì‹œë“œí‚¤ì›Œë“œ ê°œìˆ˜ ì œí•œ: {len(CUSTOM_SEED_KEYWORDS)}ê°œ â†’ {len(limited_keywords)}ê°œ")
            return limited_keywords
        return CUSTOM_SEED_KEYWORDS
    
    # 2. ìƒìˆ˜ê°€ ë¹„ì–´ìˆìœ¼ë©´ JSON íŒŒì¼ì—ì„œ ë¡œë“œ
    print(f"ğŸ“ JSON íŒŒì¼ì—ì„œ ì‹œë“œí‚¤ì›Œë“œ ë¡œë“œ: {keyword_subject}.json")
    data_dir = Path(__file__).parent.parent.parent / "data" / "keywords"
    json_file = data_dir / f"{keyword_subject}.json"
    
    if not json_file.exists():
        print(f"âŒ í‚¤ì›Œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file}")
        return []
    
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ì‹œë“œí‚¤ì›Œë“œ ë°°ì—´ì—ì„œ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
        seed_keywords = []
        for item in data.get("seed_keywords", []):
            keyword = item.get("keyword", "")
            if keyword:
                seed_keywords.append(keyword)
        
        return seed_keywords[:MAX_SEED_KEYWORDS]  # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


# ================================================================================
# ğŸ’° í™©ê¸ˆí‚¤ì›Œë“œ ë¶„ì„ ë¡œì§ (gold.py ê¸°ë°˜)
# ================================================================================

class NaverAdsClient:
    """ë„¤ì´ë²„ ê´‘ê³  API í´ë¼ì´ì–¸íŠ¸ - ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ì „ìš©"""
    
    def __init__(self, config_path: str = "config/base.yaml"):
        self.config = self._load_config(config_path)
        self.base_url = "https://api.searchad.naver.com"
        self.customer_id = self.config['credentials']['naver_ads']['customer_id']
        self.api_key = self.config['credentials']['naver_ads']['api_key']
        self.secret_key = self.config['credentials']['naver_ads']['secret_key']
        
        # ë¸”ë¡œê·¸ ìŠ¤í…Œì´ì§€ ì„¤ì • ë¡œë“œ
        self.blog_stages_config = self._load_blog_stages_config()
    
    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _load_blog_stages_config(self) -> Dict:
        """ë¸”ë¡œê·¸ ìŠ¤í…Œì´ì§€ ì„¤ì • ë¡œë“œ"""
        try:
            with open("config/keyword.yaml", 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"âš ï¸ ë¸”ë¡œê·¸ ìŠ¤í…Œì´ì§€ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _generate_signature(self, timestamp: str, method: str, uri: str) -> str:
        """ì„œëª… ìƒì„±"""
        # URIì—ì„œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±° (ì„œëª…ìš©)
        clean_uri = uri.split('?')[0] if '?' in uri else uri
        message = f"{timestamp}.{method}.{clean_uri}"
        
        signature = hmac.new(
            self.secret_key.encode('utf-8'),
            message.encode('utf-8'),
            hashlib.sha256
        ).digest()
        
        # base64 ì¸ì½”ë”©ìœ¼ë¡œ ë³€ê²½ (ë„¤ì´ë²„ API ìš”êµ¬ì‚¬í•­)
        import base64
        signature_b64 = base64.b64encode(signature).decode('utf-8')
        
        return signature_b64
    
    def _get_headers(self, method: str, uri: str) -> Dict[str, str]:
        """API ìš”ì²­ í—¤ë” ìƒì„±"""
        timestamp = str(int(time.time() * 1000))
        signature = self._generate_signature(timestamp, method, uri)
        
        headers = {
            'X-Timestamp': timestamp,
            'X-API-KEY': self.api_key,
            'X-Customer': str(self.customer_id),
            'X-Signature': signature,
            'Content-Type': 'application/json'
        }
        
        return headers
    
    def _get_search_volume_batch(self, keywords: List[str]) -> Dict[str, Dict[str, str]]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ëŸ‰ ì¡°íšŒ (5ê°œ ì´í•˜ ë°°ì¹˜) - ë””ë²„ê¹… ê°•í™”"""
        if len(keywords) > 5:
            raise ValueError("ë°°ì¹˜ë‹¹ í‚¤ì›Œë“œëŠ” ìµœëŒ€ 5ê°œê¹Œì§€ ì…ë ¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # í‚¤ì›Œë“œì—ì„œ ê³µë°± ì œê±° ë° ì •ë¦¬
        cleaned_keywords = [keyword.strip().replace(' ', '') for keyword in keywords]
        
        log_print(f"ğŸ” API ìš”ì²­ í‚¤ì›Œë“œ: {cleaned_keywords}")
        
        # API ì—”ë“œí¬ì¸íŠ¸
        uri = "/keywordstool"
        params = {
            'hintKeywords': ','.join(cleaned_keywords),
            'showDetail': 1
        }
        
        query_string = '&'.join([f"{k}={v}" for k, v in params.items()])
        full_uri = f"{uri}?{query_string}"
        
        headers = self._get_headers('GET', full_uri)
        
        try:
            response = requests.get(
                f"{self.base_url}{full_uri}",
                headers=headers
            )
            
            log_print(f"ğŸ“¡ API ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            
            response.raise_for_status()
            
            result = response.json()
            log_print(f"ğŸ“Š API ì‘ë‹µ ë°ì´í„°: {result}")
            
            # ê²€ìƒ‰ëŸ‰ë§Œ ì¶”ì¶œ
            keyword_stats = {}
            
            # ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            if isinstance(result, list):
                log_print(f"âœ… ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì‘ë‹µ - {len(result)}ê°œ í•­ëª©")
                for item in result:
                    if isinstance(item, dict):
                        keyword = item.get('relKeyword', '')
                        if keyword:
                            keyword_stats[keyword] = {
                                'pc_search_volume': item.get('monthlyPcQcCnt', '0'),
                                'mobile_search_volume': item.get('monthlyMobileQcCnt', '0')
                            }
                            log_print(f"   âœ“ {keyword}: PC={item.get('monthlyPcQcCnt')}, ëª¨ë°”ì¼={item.get('monthlyMobileQcCnt')}")
            elif isinstance(result, dict) and 'keywordList' in result:
                # keywordList í˜•íƒœì˜ ì‘ë‹µ ì²˜ë¦¬
                log_print(f"âœ… keywordList í˜•íƒœ ì‘ë‹µ - {len(result['keywordList'])}ê°œ í•­ëª©")
                for item in result['keywordList']:
                    if isinstance(item, dict):
                        keyword = item.get('relKeyword', '')
                        if keyword:
                            keyword_stats[keyword] = {
                                'pc_search_volume': item.get('monthlyPcQcCnt', '0'),
                                'mobile_search_volume': item.get('monthlyMobileQcCnt', '0')
                            }
                            log_print(f"   âœ“ {keyword}: PC={item.get('monthlyPcQcCnt')}, ëª¨ë°”ì¼={item.get('monthlyMobileQcCnt')}")
            else:
                log_print(f"âš ï¸ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì‘ë‹µ í˜•ì‹: {type(result)}")
                log_print(f"ì‘ë‹µ ë‚´ìš©: {result}")
                return {}
            
            log_print(f"ğŸ¯ ìµœì¢… ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(keyword_stats)}ê°œ")
            log_print(f"   ìš”ì²­: {cleaned_keywords}")
            log_print(f"   ì‘ë‹µ: {list(keyword_stats.keys())}")
            
            # ëˆ„ë½ëœ í‚¤ì›Œë“œ í™•ì¸
            missing = set(cleaned_keywords) - set(keyword_stats.keys())
            if missing:
                log_print(f"âŒ ëˆ„ë½ëœ í‚¤ì›Œë“œ: {missing}")
            
            return keyword_stats
            
        except requests.exceptions.RequestException as e:
            log_print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            raise Exception(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")
        except Exception as e:
            log_print(f"âŒ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise Exception(f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    def get_search_volume(self, keywords: List[str]) -> Dict[str, Dict[str, str]]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ëŸ‰ ì¡°íšŒ (ìë™ ë°°ì¹˜ ì²˜ë¦¬)"""
        # 5ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
        batches = [keywords[i:i+5] for i in range(0, len(keywords), 5)]
        
        all_results = {}
        
        for i, batch in enumerate(batches, 1):
            try:
                batch_result = self._get_search_volume_batch(batch)
                all_results.update(batch_result)
                
                # API í˜¸ì¶œ ê°„ê²© (ê³¼ë„í•œ ìš”ì²­ ë°©ì§€)
                if i < len(batches):
                    time.sleep(0.05)  # 0.05ì´ˆë¡œ ì´ˆë‹¨ì¶•
                    
            except Exception as e:
                continue
        
        return all_results
    
    def get_blog_count(self, keyword: str) -> int:
        """í‚¤ì›Œë“œì˜ ë¸”ë¡œê·¸ ë¬¸ì„œëŸ‰ ì¡°íšŒ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        max_retries = 1  # ì¬ì‹œë„ 1íšŒë§Œ
        retry_delay = 0.1  # 0.1ì´ˆ ëŒ€ê¸°ë¡œ ì´ˆë‹¨ì¶•
        
        for attempt in range(max_retries):
            try:
                # ë„¤ì´ë²„ ê²€ìƒ‰ API ì„¤ì •
                client_id = self.config['credentials']['naver_openapi']['client_id']
                client_secret = self.config['credentials']['naver_openapi']['client_secret']
                
                # URL ì¸ì½”ë”©
                enc_text = urllib.parse.quote(keyword)
                url = f"https://openapi.naver.com/v1/search/blog?query={enc_text}&display=1&start=1"
                
                # ìš”ì²­ ìƒì„±
                request = urllib.request.Request(url)
                request.add_header("X-Naver-Client-Id", client_id)
                request.add_header("X-Naver-Client-Secret", client_secret)
                
                # API í˜¸ì¶œ
                response = urllib.request.urlopen(request)
                rescode = response.getcode()
                
                if rescode == 200:
                    response_body = response.read()
                    result = json.loads(response_body.decode('utf-8'))
                    
                    # ì´ ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ ë°˜í™˜
                    total_count = result.get('total', 0)
                    return total_count
                else:
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        continue
                    return 0
                    
            except urllib.error.HTTPError as e:
                if e.code == 429:  # Too Many Requests
                    time.sleep(0.1)  # ë¹ ë¥¸ ì¬ì‹œë„
                    continue
                else:
                    return 0
            except Exception as e:
                return 0  # ë°”ë¡œ í¬ê¸°
        
        return 0
    
    def get_keyword_stage(self, doc_count: int, total_search: int) -> int:
        """í‚¤ì›Œë“œì˜ ìŠ¤í…Œì´ì§€ ë‹¨ê³„ í™•ì¸"""
        if not self.blog_stages_config or 'blog_stages' not in self.blog_stages_config:
            return 0
        
        # 1ë‹¨ê³„ë¶€í„° 5ë‹¨ê³„ê¹Œì§€ ìˆœì„œëŒ€ë¡œ í™•ì¸
        for stage_num in range(1, 6):
            stage_key = str(stage_num)
            if stage_key in self.blog_stages_config['blog_stages']:
                stage_config = self.blog_stages_config['blog_stages'][stage_key]
                d_max = stage_config.get('D_max', float('inf'))
                s_min = stage_config.get('S_min', 0)
                s_max = stage_config.get('S_max', float('inf'))
                
                # ê¸°ë³¸ ì¡°ê±´ í™•ì¸
                if (doc_count <= d_max and s_min <= total_search <= s_max):
                    # 2~5ë‹¨ê³„ì—ì„œëŠ” ì¶”ê°€ ì¡°ê±´: ê²€ìƒ‰ëŸ‰ >= ë¬¸ì„œìˆ˜
                    if stage_num == 1:
                        # 1ë‹¨ê³„ëŠ” ê¸°ë³¸ ì¡°ê±´ë§Œ í™•ì¸
                        return stage_num
                    else:
                        # 2~5ë‹¨ê³„ëŠ” ê²€ìƒ‰ëŸ‰ì´ ë¬¸ì„œìˆ˜ë³´ë‹¤ ë†’ê±°ë‚˜ ê°™ì•„ì•¼ í•¨
                        if total_search >= doc_count:
                            return stage_num
                        # ì¡°ê±´ì— ë§ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ í™•ì¸
                        continue
        
        return 0  # ì–´ë–¤ ë‹¨ê³„ì—ë„ í•´ë‹¹í•˜ì§€ ì•ŠìŒ

    def get_keyword_analysis(self, keywords: List[str]) -> Dict[str, Dict[str, any]]:
        """í‚¤ì›Œë“œ í†µí•© ë¶„ì„ (ê²€ìƒ‰ëŸ‰ + ë¬¸ì„œëŸ‰ + ê²½ìŸë„) - ë°°ì¹˜ ì²˜ë¦¬"""
        
        # ê²€ìƒ‰ëŸ‰ ì¡°íšŒ
        search_volume_result = self.get_search_volume(keywords)
        
        # í†µí•© ê²°ê³¼ ìƒì„±
        analysis_result = {}
        
        # í‚¤ì›Œë“œ ì •ë¦¬ (ê³µë°± ì œê±°)
        cleaned_keywords = [keyword.strip().replace(' ', '') for keyword in keywords]
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (50ê°œì”©)
        batch_size = 50
        total_batches = (len(keywords) + batch_size - 1) // batch_size
        
        for batch_idx in range(0, len(keywords), batch_size):
            batch_keywords = keywords[batch_idx:batch_idx + batch_size]
            batch_num = (batch_idx // batch_size) + 1
            
            for i, original_keyword in enumerate(batch_keywords):
                global_i = batch_idx + i
                cleaned_keyword = cleaned_keywords[global_i]
                
                if cleaned_keyword in search_volume_result:
                    # ë¸”ë¡œê·¸ ë¬¸ì„œëŸ‰ ì¡°íšŒ (ì›ë³¸ í‚¤ì›Œë“œë¡œ)
                    blog_count = self.get_blog_count(original_keyword)
                    
                    # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ê°„ê²©
                    time.sleep(0.02)  # ê° í‚¤ì›Œë“œë§ˆë‹¤ 0.02ì´ˆë§Œ ëŒ€ê¸°
                    
                    # PCì™€ ëª¨ë°”ì¼ ê²€ìƒ‰ëŸ‰ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (< 10 ê°™ì€ ë¬¸ìì—´ ì²˜ë¦¬)
                    def parse_volume(volume_str):
                        if isinstance(volume_str, int):
                            return volume_str
                        if isinstance(volume_str, str):
                            if volume_str.startswith('<'):
                                return 5  # "< 10"ì¸ ê²½ìš° 5ë¡œ ì²˜ë¦¬
                            try:
                                return int(volume_str)
                            except:
                                return 0
                        return 0
                    
                    pc_volume = parse_volume(search_volume_result[cleaned_keyword]['pc_search_volume'])
                    mobile_volume = parse_volume(search_volume_result[cleaned_keyword]['mobile_search_volume'])
                    
                    # ì´ê²€ìƒ‰ëŸ‰ ê³„ì‚°
                    total_search_volume = pc_volume + mobile_volume
                    
                    # ê²½ìŸë„ ê³„ì‚° (ë¬¸ì„œìˆ˜ Ã· ì´ê²€ìƒ‰ëŸ‰)
                    competition_ratio = round(blog_count / total_search_volume, 3) if total_search_volume > 0 else 0
                    
                    analysis_result[original_keyword] = {
                        'pc_search_volume': pc_volume,
                        'mobile_search_volume': mobile_volume,
                        'total_search_volume': total_search_volume,
                        'blog_count': blog_count,
                        'competition_ratio': competition_ratio
                    }
            
            # ë°°ì¹˜ ê°„ ëŒ€ê¸° (ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ í›„)
            if batch_num < total_batches:
                time.sleep(0.1)  # 0.1ì´ˆë¡œ ì´ˆë‹¨ì¶•
        
        return analysis_result
    
    def get_keyword_analysis_with_save(self, keywords: List[str]) -> Dict[str, Dict[str, any]]:
        """í‚¤ì›Œë“œ í†µí•© ë¶„ì„ - ì ì§„ì  ì €ì¥ (ê°•ì œì¢…ë£Œ ì‹œ ì•ˆì „)"""
        
        # ê²€ìƒ‰ëŸ‰ ì¡°íšŒ
        search_volume_result = self.get_search_volume(keywords)
        
        # í†µí•© ê²°ê³¼ ì €ì¥
        analysis_result = {}
        
        # í‚¤ì›Œë“œ ì •ë¦¬ (ê³µë°± ì œê±°)
        cleaned_keywords = [keyword.strip().replace(' ', '') for keyword in keywords]
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (20ê°œì”©ìœ¼ë¡œ ì¤„ì—¬ì„œ ë” ìì£¼ ì €ì¥)
        batch_size = 20
        total_batches = (len(keywords) + batch_size - 1) // batch_size
        
        try:
            for batch_idx in range(0, len(keywords), batch_size):
                batch_keywords = keywords[batch_idx:batch_idx + batch_size]
                batch_num = (batch_idx // batch_size) + 1
                
                for i, original_keyword in enumerate(batch_keywords):
                    global_i = batch_idx + i
                    cleaned_keyword = cleaned_keywords[global_i]
                    
                    # ë¸”ë¡œê·¸ ë¬¸ì„œëŸ‰ ì¡°íšŒ (ì›ë³¸ í‚¤ì›Œë“œë¡œ) - ëª¨ë“  í‚¤ì›Œë“œì— ëŒ€í•´ ì‹¤í–‰
                    blog_count = self.get_blog_count(original_keyword)
                    
                    # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ê°„ê²©
                    time.sleep(0.02)  # ê° í‚¤ì›Œë“œë§ˆë‹¤ 0.02ì´ˆë§Œ ëŒ€ê¸°
                    
                    # PCì™€ ëª¨ë°”ì¼ ê²€ìƒ‰ëŸ‰ì„ ì •ìˆ˜ë¡œ ë³€í™˜ (< 10 ê°™ì€ ë¬¸ìì—´ ì²˜ë¦¬)
                    def parse_volume(volume_str):
                        if isinstance(volume_str, int):
                            return volume_str
                        if isinstance(volume_str, str):
                            if volume_str.startswith('<'):
                                return 5  # "< 10"ì¸ ê²½ìš° 5ë¡œ ì²˜ë¦¬
                            try:
                                return int(volume_str)
                            except:
                                return 0
                        return 0
                    
                    # ê²€ìƒ‰ëŸ‰ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì„¤ì •
                    if cleaned_keyword in search_volume_result:
                        pc_volume = parse_volume(search_volume_result[cleaned_keyword]['pc_search_volume'])
                        mobile_volume = parse_volume(search_volume_result[cleaned_keyword]['mobile_search_volume'])
                    else:
                        pc_volume = 0
                        mobile_volume = 0
                    
                    # ì´ê²€ìƒ‰ëŸ‰ ê³„ì‚°
                    total_search_volume = pc_volume + mobile_volume
                    
                    # ê²½ìŸë„ ê³„ì‚° (ë¬¸ì„œìˆ˜ Ã· ì´ê²€ìƒ‰ëŸ‰)
                    competition_ratio = round(blog_count / total_search_volume, 3) if total_search_volume > 0 else 0
                    
                    analysis_result[original_keyword] = {
                        'pc_search_volume': pc_volume,
                        'mobile_search_volume': mobile_volume,
                        'total_search_volume': total_search_volume,
                        'blog_count': blog_count,
                        'competition_ratio': competition_ratio
                    }
                
                # ë°°ì¹˜ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥ (í™©ê¸ˆí‚¤ì›Œë“œê°€ ìˆìœ¼ë©´)
                if analysis_result:
                    self._save_intermediate_results(analysis_result)
                
                # ë°°ì¹˜ ê°„ ëŒ€ê¸° (ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ í›„)
                if batch_num < total_batches:
                    time.sleep(0.1)  # 0.1ì´ˆë¡œ ì´ˆë‹¨ì¶•
            
            return analysis_result
            
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ ë¶„ì„ ì¤‘ë‹¨ë¨ - ì§€ê¸ˆê¹Œì§€ ë¶„ì„ëœ {len(analysis_result)}ê°œ í‚¤ì›Œë“œ ì €ì¥ ì¤‘...")
            if analysis_result:
                self._save_final_results(analysis_result)
            raise
    
    def _save_intermediate_results(self, analysis_result: Dict[str, Dict[str, any]]):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (í™©ê¸ˆí‚¤ì›Œë“œë§Œ)"""
        try:
            # íƒ€ê²Ÿ ìŠ¤í…Œì´ì§€ ì´ë‚´ í•„í„°ë§ (ê¸°ë³¸/ìë™ ëª¨ë“œ ê³µí†µ)
            filtered_result = self.filter_keywords_by_target_stages(analysis_result, TARGET_STAGES)
            
            # í™©ê¸ˆí‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì €ì¥
            if filtered_result:
                self.save_to_excel(filtered_result)
        except:
            pass  # ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    
    def _save_final_results(self, analysis_result: Dict[str, Dict[str, any]]):
        """ìµœì¢… ê²°ê³¼ ì €ì¥ (ê°•ì œ ì¢…ë£Œ ì‹œ)"""
        try:
            # íƒ€ê²Ÿ ìŠ¤í…Œì´ì§€ ì´ë‚´ í•„í„°ë§ (ê¸°ë³¸/ìë™ ëª¨ë“œ ê³µí†µ)
            filtered_result = self.filter_keywords_by_target_stages(analysis_result, TARGET_STAGES)
            
            # ê²°ê³¼ ì €ì¥
            if filtered_result:
                filename = self.save_to_excel(filtered_result)
                print(f"âœ… ì¤‘ë‹¨ ì‹œì ê¹Œì§€ {len(filtered_result)}ê°œ í™©ê¸ˆí‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {filename}")
            else:
                print("âŒ ì €ì¥í•  í™©ê¸ˆí‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ìµœì¢… ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def filter_keywords_by_stage(self, analysis_result: Dict[str, Dict[str, any]], stage: int = None) -> Dict[str, Dict[str, any]]:
        """ë¸”ë¡œê·¸ ìŠ¤í…Œì´ì§€ ì¡°ê±´ì— ë§ëŠ” í‚¤ì›Œë“œë§Œ í•„í„°ë§"""
        if stage is None:
            # ê¸°ë³¸ëª¨ë“œì—ì„œëŠ” TARGET_STAGESë¥¼ ì‚¬ìš© (ë” ì´ìƒ ë‹¨ì¼ stageê°€ ì•„ë‹˜)
            return self.filter_keywords_by_target_stages(analysis_result, TARGET_STAGES)
        
        if not self.blog_stages_config or 'blog_stages' not in self.blog_stages_config:
            print("âš ï¸ ë¸”ë¡œê·¸ ìŠ¤í…Œì´ì§€ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return analysis_result
        
        stage_key = str(stage)
        if stage_key not in self.blog_stages_config['blog_stages']:
            print(f"âš ï¸ ìŠ¤í…Œì´ì§€ {stage} ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return analysis_result
        
        stage_config = self.blog_stages_config['blog_stages'][stage_key]
        d_max = stage_config.get('D_max', float('inf'))  # ë¬¸ì„œìˆ˜ ìµœëŒ€ê°’
        s_min = stage_config.get('S_min', 0)            # ê²€ìƒ‰ëŸ‰ ìµœì†Œê°’
        s_max = stage_config.get('S_max', float('inf')) # ê²€ìƒ‰ëŸ‰ ìµœëŒ€ê°’
        
        print(f"ğŸ” ìŠ¤í…Œì´ì§€ {stage} í•„í„°ë§ ì¡°ê±´:")
        print(f"   ğŸ“„ ë¬¸ì„œìˆ˜: â‰¤ {d_max}")
        print(f"   ğŸ” ê²€ìƒ‰ëŸ‰: {s_min} ~ {s_max}")
        
        filtered_result = {}
        total_count = len(analysis_result)
        passed_count = 0
        
        for keyword, stats in analysis_result.items():
            doc_count = stats['blog_count']
            total_search = stats['total_search_volume']
            
            # í‚¤ì›Œë“œê°€ ì†í•˜ëŠ” ìŠ¤í…Œì´ì§€ í™•ì¸
            keyword_stage = self.get_keyword_stage(doc_count, total_search)
            
            # í˜„ì¬ ì„¤ì •ëœ ìŠ¤í…Œì´ì§€ ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸
            if (doc_count <= d_max and 
                s_min <= total_search <= s_max):
                # ìŠ¤í…Œì´ì§€ ì •ë³´ ì¶”ê°€
                stats['stage'] = keyword_stage
                filtered_result[keyword] = stats
                passed_count += 1
        
        print(f"âœ… í•„í„°ë§ ì™„ë£Œ: {total_count}ê°œ ì¤‘ {passed_count}ê°œ í‚¤ì›Œë“œ í†µê³¼")
        return filtered_result
    
    def filter_keywords_auto_mode(self, analysis_result: Dict[str, Dict[str, any]], target_stages: List[int] = None) -> Dict[str, Dict[str, any]]:
        """ìë™ëª¨ë“œ: ì§€ì •ëœ ë‹¨ê³„ì— í•´ë‹¹í•˜ëŠ” í‚¤ì›Œë“œë§Œ ë¶„ë¥˜í•˜ì—¬ ì €ì¥"""
        if not self.blog_stages_config or 'blog_stages' not in self.blog_stages_config:
            return analysis_result
        
        # ê¸°ë³¸ê°’ì€ TARGET_STAGES ì‚¬ìš©
        if target_stages is None:
            target_stages = TARGET_STAGES
        
        auto_result = {}
        
        for keyword, stats in analysis_result.items():
            doc_count = stats['blog_count']
            total_search = stats['total_search_volume']
            
            # í‚¤ì›Œë“œê°€ ì†í•˜ëŠ” ìŠ¤í…Œì´ì§€ í™•ì¸
            keyword_stage = self.get_keyword_stage(doc_count, total_search)
            
            # ëª©í‘œ ë‹¨ê³„ì— í•´ë‹¹í•˜ëŠ” í‚¤ì›Œë“œë§Œ ì €ì¥
            if keyword_stage in target_stages:
                stats['stage'] = keyword_stage
                auto_result[keyword] = stats
        
        return auto_result
    
    def filter_keywords_by_target_stages(self, analysis_result: Dict[str, Dict[str, any]], target_stages: List[int] = None) -> Dict[str, Dict[str, any]]:
        """íƒ€ê²Ÿ ìŠ¤í…Œì´ì§€ ì´ë‚´ í‚¤ì›Œë“œ í•„í„°ë§ (ê¸°ë³¸/ìë™ ëª¨ë“œ ê³µí†µ)"""
        if target_stages is None:
            target_stages = TARGET_STAGES
        
        if not self.blog_stages_config or 'blog_stages' not in self.blog_stages_config:
            print("âŒ ë¸”ë¡œê·¸ ìŠ¤í…Œì´ì§€ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        filtered_result = {}
        
        for keyword, data in analysis_result.items():
            # í•´ë‹¹ í‚¤ì›Œë“œì˜ ìŠ¤í…Œì´ì§€ ë¶„ë¥˜
            keyword_stage = self.get_keyword_stage(data['blog_count'], data['total_search_volume'])
            
            # íƒ€ê²Ÿ ìŠ¤í…Œì´ì§€ ì´ë‚´ì— í¬í•¨ë˜ë©´ í•„í„°ë§
            if keyword_stage in target_stages:
                filtered_result[keyword] = data
                filtered_result[keyword]['stage'] = keyword_stage
        
        return filtered_result
    
    def save_to_excel(self, analysis_result: Dict[str, Dict[str, any]], filename: str = None) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ (ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€/ì—…ë°ì´íŠ¸)"""
        try:
            # ì €ì¥ í´ë” ìƒì„±
            save_directory = "static/gold_keyword"
            os.makedirs(save_directory, exist_ok=True)
            
            # íŒŒì¼ëª… ì„¤ì •
            if filename is None:
                filename = f"{KEYWORD_SUBJECT}.xlsx"
            
            # ì „ì²´ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            filepath = os.path.join(save_directory, filename)
            
            # ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            existing_df = None
            if os.path.exists(filepath):
                try:
                    existing_df = pd.read_excel(filepath)
                except Exception as e:
                    pass
            
            # ìƒˆë¡œìš´ ë°ì´í„° ì¤€ë¹„
            new_data = []
            for keyword, stats in analysis_result.items():
                new_data.append({
                    'í‚¤ì›Œë“œ': keyword,
                    'PC ê²€ìƒ‰ëŸ‰': stats['pc_search_volume'],
                    'ëª¨ë°”ì¼ ê²€ìƒ‰ëŸ‰': stats['mobile_search_volume'],
                    'ì´ê²€ìƒ‰ëŸ‰': stats['total_search_volume'],
                    'ë¬¸ì„œëŸ‰': stats['blog_count'],
                    'ê²½ìŸë„': stats['competition_ratio'],
                    'ë‹¨ê³„': stats.get('stage', 0)  # ìŠ¤í…Œì´ì§€ ì •ë³´ ì¶”ê°€
                })
            
            new_df = pd.DataFrame(new_data)
            
            if existing_df is not None:
                # ìë™ëª¨ë“œì—ì„œ ê¸°ì¡´ ë°ì´í„°ì˜ 0ë‹¨ê³„ í‚¤ì›Œë“œ ì œê±°
                if MODE == "AUTO":
                    # ê¸°ì¡´ ë°ì´í„°ì—ì„œ 0ë‹¨ê³„ë‚˜ NaN ë‹¨ê³„ í‚¤ì›Œë“œ ì œê±°
                    if 'ë‹¨ê³„' in existing_df.columns:
                        existing_df = existing_df[(existing_df['ë‹¨ê³„'] > 0) & (existing_df['ë‹¨ê³„'].notna())]
                
                # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ë³‘í•©
                # í‚¤ì›Œë“œê°€ ì¤‘ë³µë˜ë©´ ìƒˆ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                
                # ì¤‘ë³µ í‚¤ì›Œë“œ ì œê±° (ë§ˆì§€ë§‰ ë°ì´í„° ìœ ì§€)
                combined_df = combined_df.drop_duplicates(subset=['í‚¤ì›Œë“œ'], keep='last')
            else:
                combined_df = new_df
            
            # ê²½ìŸë„(ë¹„ìœ¨)ê°€ ë‚®ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            combined_df = combined_df.sort_values('ê²½ìŸë„', ascending=True)
            
            # ì—‘ì…€ íŒŒì¼ ì €ì¥
            combined_df.to_excel(filepath, index=False, engine='openpyxl')
            
            return filepath
            
        except Exception as e:
            raise Exception(f"ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def save_all_keywords_to_test(self, analysis_result: Dict[str, Dict[str, any]], filename: str = None) -> str:
        """í•„í„°ë§ ì „ ì „ì²´ ì—°ê´€í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ static/test/ì— ì €ì¥"""
        try:
            # ì €ì¥ í´ë” ìƒì„±
            save_directory = "static/test"
            os.makedirs(save_directory, exist_ok=True)
            
            # íŒŒì¼ëª… ì„¤ì • (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{KEYWORD_SUBJECT}_ì „ì²´ì—°ê´€í‚¤ì›Œë“œ_{timestamp}.xlsx"
            
            # ì „ì²´ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            filepath = os.path.join(save_directory, filename)
            
            # ìƒˆë¡œìš´ ë°ì´í„° ì¤€ë¹„ (ëª¨ë“  í‚¤ì›Œë“œ, ìŠ¤í…Œì´ì§€ ì •ë³´ í¬í•¨)
            all_data = []
            for keyword, stats in analysis_result.items():
                # í‚¤ì›Œë“œì˜ ìŠ¤í…Œì´ì§€ í™•ì¸
                keyword_stage = self.get_keyword_stage(stats['blog_count'], stats['total_search_volume'])
                
                all_data.append({
                    'í‚¤ì›Œë“œ': keyword,
                    'PC ê²€ìƒ‰ëŸ‰': stats['pc_search_volume'],
                    'ëª¨ë°”ì¼ ê²€ìƒ‰ëŸ‰': stats['mobile_search_volume'],
                    'ì´ê²€ìƒ‰ëŸ‰': stats['total_search_volume'],
                    'ë¬¸ì„œëŸ‰': stats['blog_count'],
                    'ê²½ìŸë„': stats['competition_ratio'],
                    'ë‹¨ê³„': keyword_stage
                })
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(all_data)
            
            # ê²½ìŸë„(ë¹„ìœ¨)ê°€ ë‚®ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            df = df.sort_values('ê²½ìŸë„', ascending=True)
            
            # ì—‘ì…€ íŒŒì¼ ì €ì¥
            df.to_excel(filepath, index=False, engine='openpyxl')
            
            return filepath
            
        except Exception as e:
            raise Exception(f"ì „ì²´ í‚¤ì›Œë“œ í…ŒìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def debug_search_volume(self, test_keywords: List[str]) -> None:
        """íŠ¹ì • í‚¤ì›Œë“œë“¤ì˜ ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ë””ë²„ê¹…"""
        log_print(f"\nğŸ”§ ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ë””ë²„ê¹… ì‹œì‘ - {len(test_keywords)}ê°œ í‚¤ì›Œë“œ")
        log_print("=" * 60)
        
        for keyword in test_keywords:
            log_print(f"\nğŸ¯ í‚¤ì›Œë“œ: '{keyword}'")
            try:
                # ë‹¨ì¼ í‚¤ì›Œë“œë¡œ API í˜¸ì¶œ
                result = self._get_search_volume_batch([keyword])
                if result:
                    log_print(f"âœ… ì„±ê³µ: {result}")
                else:
                    log_print("âŒ ì‘ë‹µ ì—†ìŒ")
            except Exception as e:
                log_print(f"âŒ ì˜¤ë¥˜: {e}")
            
            log_print("-" * 40)


# ================================================================================
# ğŸ¤– ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ ë¡œì§
# ================================================================================

def get_available_subjects() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ì£¼ì œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    data_dir = Path(__file__).parent.parent.parent / "data" / "keywords"
    available_subjects = []
    
    for json_file in data_dir.glob("*.json"):
        if json_file.stem not in ["", "template"]:  # í…œí”Œë¦¿ íŒŒì¼ ì œì™¸
            available_subjects.append(json_file.stem)
    
    return available_subjects


def select_random_subject() -> str:
    """ëœë¤ ì£¼ì œ ì„ íƒ"""
    if AUTO_RANDOM_SUBJECTS:
        # ì§€ì •ëœ ì£¼ì œ ëª©ë¡ì—ì„œ ì„ íƒ
        return random.choice(AUTO_RANDOM_SUBJECTS)
    else:
        # ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ ì£¼ì œì—ì„œ ì„ íƒ
        available_subjects = get_available_subjects()
        if available_subjects:
            return random.choice(available_subjects)
        else:
            return KEYWORD_SUBJECT  # ê¸°ë³¸ê°’ ì‚¬ìš©


def run_single_analysis(subject: str) -> bool:
    """ë‹¨ì¼ ì£¼ì œì— ëŒ€í•œ í™©ê¸ˆí‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰"""
    analysis_result = None
    client = None
    
    try:
        print(f"\nğŸ” ì£¼ì œ '{subject}' ë¶„ì„ ì‹œì‘...")
        
        # 1ë‹¨ê³„: ì‹œë“œí‚¤ì›Œë“œ ë¡œë“œ
        seed_keywords = load_seed_keywords_from_json(subject)
        if not seed_keywords:
            print(f"âŒ '{subject}' ì‹œë“œí‚¤ì›Œë“œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # 2ë‹¨ê³„: ì—°ê´€í‚¤ì›Œë“œ í™•ì¥
        all_keywords = expand_keywords_recursive(seed_keywords, max_keywords=MAX_KEYWORDS)
        if not all_keywords:
            print(f"âŒ '{subject}' ì—°ê´€í‚¤ì›Œë“œ í™•ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
        
        # 2-1ë‹¨ê³„: ì—°ê´€í‚¤ì›Œë“œ ì›ë³¸ ì €ì¥ (ë¶„ì„ ì „)
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            raw_keywords_filename = f"static/test/{subject}_ì—°ê´€í‚¤ì›Œë“œì›ë³¸_{timestamp}.xlsx"
            os.makedirs("static/test", exist_ok=True)
            
            # ë‹¨ìˆœíˆ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë§Œ ì €ì¥
            raw_df = pd.DataFrame({'í‚¤ì›Œë“œ': all_keywords})
            raw_df.to_excel(raw_keywords_filename, index=False, engine='openpyxl')
            print(f"ğŸ’¾ '{subject}' ì—°ê´€í‚¤ì›Œë“œ ì›ë³¸ ì €ì¥ ì™„ë£Œ: {raw_keywords_filename}")
            print(f"ğŸ“Š ì—°ê´€í‚¤ì›Œë“œ ì›ë³¸ ìˆ˜: {len(all_keywords)}ê°œ")
        except Exception as e:
            print(f"âš ï¸ '{subject}' ì—°ê´€í‚¤ì›Œë“œ ì›ë³¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # 3ë‹¨ê³„: í™©ê¸ˆí‚¤ì›Œë“œ ë¶„ì„
        client = NaverAdsClient()
        analysis_result = client.get_keyword_analysis(all_keywords)
        
        if not analysis_result:
            print(f"âŒ '{subject}' í‚¤ì›Œë“œ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
        
        # 3-1ë‹¨ê³„: í•„í„°ë§ ì „ ì „ì²´ ì—°ê´€í‚¤ì›Œë“œ ë°ì´í„° ì €ì¥ (í…ŒìŠ¤íŠ¸ìš©)
        try:
            test_filename = client.save_all_keywords_to_test(analysis_result, f"{subject}_ì „ì²´ì—°ê´€í‚¤ì›Œë“œ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")
            print(f"ğŸ’¾ '{subject}' í•„í„°ë§ ì „ ì „ì²´ ì—°ê´€í‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {test_filename}")
            print(f"ğŸ“Š ì „ì²´ ì—°ê´€í‚¤ì›Œë“œ ìˆ˜: {len(analysis_result)}ê°œ")
        except Exception as e:
            print(f"âš ï¸ '{subject}' ì „ì²´ ì—°ê´€í‚¤ì›Œë“œ í…ŒìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # 4ë‹¨ê³„: ìŠ¤í…Œì´ì§€ í•„í„°ë§ (íƒ€ê²Ÿ ìŠ¤í…Œì´ì§€ ì´ë‚´)
        filtered_result = client.filter_keywords_by_target_stages(analysis_result, TARGET_STAGES)
        
        # 5ë‹¨ê³„: ì—‘ì…€ íŒŒì¼ ì €ì¥
        if filtered_result:
            filename = client.save_to_excel(filtered_result, f"{subject}.xlsx")
            print(f"âœ… '{subject}' í™©ê¸ˆí‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {filename}")
            print(f"ğŸ“Š í™©ê¸ˆí‚¤ì›Œë“œ ìˆ˜: {len(filtered_result)}ê°œ")
            return True
        else:
            print(f"âŒ '{subject}' í™©ê¸ˆí‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
            
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ '{subject}' ë¶„ì„ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ì§„í–‰ëœ ë¶€ë¶„ê¹Œì§€ ì €ì¥
        if analysis_result and client:
            try:
                print(f"ğŸ’¾ ì§„í–‰ëœ '{subject}' ë°ì´í„° ì €ì¥ ì¤‘...")
                filtered_result = client.filter_keywords_by_target_stages(analysis_result, TARGET_STAGES)
                if filtered_result:
                    filename = client.save_to_excel(filtered_result, f"{subject}.xlsx")
                    print(f"âœ… ì¤‘ë‹¨ëœ '{subject}' í™©ê¸ˆí‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {filename}")
                    print(f"ğŸ“Š ì €ì¥ëœ í‚¤ì›Œë“œ ìˆ˜: {len(filtered_result)}ê°œ")
                else:
                    print(f"âš ï¸ '{subject}' ì €ì¥í•  í™©ê¸ˆí‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as save_error:
                print(f"âŒ '{subject}' ì¤‘ë‹¨ í›„ ì €ì¥ ì‹¤íŒ¨: {save_error}")
        raise  # KeyboardInterruptë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìƒìœ„ì—ì„œ ì²˜ë¦¬
            
    except Exception as e:
        print(f"âŒ '{subject}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì§„í–‰ëœ ë¶€ë¶„ê¹Œì§€ ì €ì¥ ì‹œë„
        if analysis_result and client:
            try:
                print(f"ğŸ’¾ ì˜¤ë¥˜ ë°œìƒ, '{subject}' ì§„í–‰ëœ ë°ì´í„° ì €ì¥ ì‹œë„...")
                filtered_result = client.filter_keywords_auto_mode(analysis_result, TARGET_STAGES)
                if filtered_result:
                    filename = client.save_to_excel(filtered_result, f"{subject}.xlsx")
                    print(f"âœ… ì˜¤ë¥˜ í›„ '{subject}' í™©ê¸ˆí‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {filename}")
                    print(f"ğŸ“Š ì €ì¥ëœ í‚¤ì›Œë“œ ìˆ˜: {len(filtered_result)}ê°œ")
            except Exception as save_error:
                print(f"âŒ '{subject}' ì˜¤ë¥˜ í›„ ì €ì¥ ì‹¤íŒ¨: {save_error}")
        return False


def background_auto_worker():
    """ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ ì›Œì»¤ í•¨ìˆ˜"""
    print(f"ğŸ¤– ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ ì‹œì‘ - {AUTO_CYCLE_MINUTES}ë¶„ ì£¼ê¸°ë¡œ ì‹¤í–‰")
    print(f"ğŸ¯ ëª©í‘œ ìŠ¤í…Œì´ì§€: {TARGET_STAGES}ë‹¨ê³„")
    
    cycle_count = 0
    
    while True:
        try:
            cycle_count += 1
            print(f"\n{'='*60}")
            print(f"ğŸ”„ ìë™ ì‚¬ì´í´ #{cycle_count} ì‹œì‘")
            print(f"{'='*60}")
            
            # ëœë¤ ì£¼ì œ ì„ íƒ
            selected_subject = select_random_subject()
            print(f"ğŸ² ì„ íƒëœ ì£¼ì œ: '{selected_subject}'")
            
            # í™©ê¸ˆí‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰
            success = run_single_analysis(selected_subject)
            
            if success:
                print(f"ğŸ‰ ì‚¬ì´í´ #{cycle_count} ì™„ë£Œ: '{selected_subject}' í™©ê¸ˆí‚¤ì›Œë“œ ìƒì„± ì„±ê³µ!")
            else:
                print(f"âš ï¸ ì‚¬ì´í´ #{cycle_count} ì‹¤íŒ¨: '{selected_subject}' í™©ê¸ˆí‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨")
            
            # ë‹¤ìŒ ì‹¤í–‰ê¹Œì§€ ëŒ€ê¸°
            print(f"\nğŸ˜´ {AUTO_CYCLE_MINUTES}ë¶„ í›„ ë‹¤ìŒ ì‚¬ì´í´ ì‹¤í–‰...")
            time.sleep(AUTO_CYCLE_MINUTES * 60)
            
        except KeyboardInterrupt:
            print("\nğŸ›‘ ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ ì¤‘ë‹¨ë¨")
            break
        except Exception as e:
            print(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ ì—ëŸ¬: {e}")
            print(f"â±ï¸ 5ë¶„ í›„ ì¬ì‹œë„...")
            time.sleep(300)  # 5ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„


def background_auto_mode():
    """ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ ì‹œì‘"""
    print("ğŸ¤– ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ ì‹œì‘")
    print("ğŸ›‘ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    
    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
    worker_thread = Thread(target=background_auto_worker, daemon=True)
    worker_thread.start()
    
    try:
        worker_thread.join()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")


# ================================================================================
# ğŸš€ í†µí•© ì‹¤í–‰ ë¡œì§
# ================================================================================

def main():
    """í†µí•© í™©ê¸ˆí‚¤ì›Œë“œ ë„ì¶œ ë©”ì¸ í•¨ìˆ˜"""
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    if len(sys.argv) > 1:
        mode_arg = sys.argv[1].lower()
        if mode_arg == "background":
            # ë°±ê·¸ë¼ìš´ë“œ ìë™ëª¨ë“œ ì‹¤í–‰
            background_auto_mode()
            return 0
    
    # ë°±ê·¸ë¼ìš´ë“œ ëª¨ë“œ ì„¤ì •ì´ë©´ ìë™ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    if MODE == "BACKGROUND":
        background_auto_mode()
        return 0
    
    analysis_result = None
    client = None
    
    try:
        # 1. ì„ íƒí•œ ì‹œë“œí‚¤ì›Œë“œ
        seed_keywords = load_seed_keywords_from_json(KEYWORD_SUBJECT)
        if not seed_keywords:
            print("âŒ ì‹œë“œí‚¤ì›Œë“œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 1
        
        print(f"1. ì„ íƒí•œ ì‹œë“œí‚¤ì›Œë“œ: {seed_keywords[0]}")
        
        # 2. ì—°ê´€í‚¤ì›Œë“œ í™•ì¥
        all_keywords = expand_keywords_recursive(seed_keywords)
        if not all_keywords:
            print("âŒ ì—°ê´€í‚¤ì›Œë“œ í™•ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return 1
        
        print(f"2. ì—°ê´€í‚¤ì›Œë“œ ê°œìˆ˜: {len(all_keywords)}ê°œ")
        
        # 2-1. ì—°ê´€í‚¤ì›Œë“œ ì›ë³¸ ì €ì¥ (ë¶„ì„ ì „)
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            raw_keywords_filename = f"static/test/{KEYWORD_SUBJECT}_ì—°ê´€í‚¤ì›Œë“œì›ë³¸_{timestamp}.xlsx"
            os.makedirs("static/test", exist_ok=True)
            
            # ë‹¨ìˆœíˆ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë§Œ ì €ì¥
            raw_df = pd.DataFrame({'í‚¤ì›Œë“œ': all_keywords})
            raw_df.to_excel(raw_keywords_filename, index=False, engine='openpyxl')
            print(f"ğŸ’¾ ì—°ê´€í‚¤ì›Œë“œ ì›ë³¸ ì €ì¥ ì™„ë£Œ: {raw_keywords_filename}")
            print(f"ğŸ“Š ì—°ê´€í‚¤ì›Œë“œ ì›ë³¸ ìˆ˜: {len(all_keywords)}ê°œ")
        except Exception as e:
            print(f"âš ï¸ ì—°ê´€í‚¤ì›Œë“œ ì›ë³¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # 3. í™©ê¸ˆí‚¤ì›Œë“œ ë¶„ì„ (ì ì§„ì  ì €ì¥)
        client = NaverAdsClient()
        analysis_result = client.get_keyword_analysis_with_save(all_keywords)
        
        if not analysis_result:
            print("âŒ í‚¤ì›Œë“œ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return 1
        
        # 3-1. í•„í„°ë§ ì „ ì „ì²´ ì—°ê´€í‚¤ì›Œë“œ ë°ì´í„° ì €ì¥ (í…ŒìŠ¤íŠ¸ìš©)
        try:
            test_filename = client.save_all_keywords_to_test(analysis_result)
            print(f"ğŸ’¾ í•„í„°ë§ ì „ ì „ì²´ ì—°ê´€í‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {test_filename}")
            print(f"ğŸ“Š ì „ì²´ ì—°ê´€í‚¤ì›Œë“œ ìˆ˜: {len(analysis_result)}ê°œ")
        except Exception as e:
            print(f"âš ï¸ ì „ì²´ ì—°ê´€í‚¤ì›Œë“œ í…ŒìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # 4. ìŠ¤í…Œì´ì§€ í•„í„°ë§ (ê¸°ë³¸/ìë™ ëª¨ë“œ ëª¨ë‘ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)
        filtered_result = client.filter_keywords_by_target_stages(analysis_result, TARGET_STAGES)
        
        # 3. í•„í„°ë§ í›„ í™©ê¸ˆí‚¤ì›Œë“œ ëª©ë¡
        print(f"3. í•„í„°ë§ í›„ í™©ê¸ˆí‚¤ì›Œë“œ ëª©ë¡ ({len(filtered_result)}ê°œ):")
        print("=" * 100)
        print(f"{'í‚¤ì›Œë“œ':<20} {'ì´ê²€ìƒ‰ëŸ‰':<10} {'ë¬¸ì„œëŸ‰':<10} {'ê²½ìŸë„':<10} {'ë‹¨ê³„':<5}")
        print("-" * 100)
        
        if filtered_result:
            # ê²½ìŸë„ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
            sorted_keywords = sorted(filtered_result.items(), key=lambda x: x[1]['competition_ratio'])
            
            for keyword, stats in sorted_keywords:
                stage = stats.get('stage', 0)
                stage_text = f"{stage}ë‹¨ê³„" if stage > 0 else "í•´ë‹¹ì—†ìŒ"
                print(f"{keyword:<20} {stats['total_search_volume']:<10} {stats['blog_count']:<10} {stats['competition_ratio']:<10} {stage_text:<5}")
        else:
            print("âŒ í™©ê¸ˆí‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì—‘ì…€ íŒŒì¼ ì €ì¥
        try:
            filename = client.save_to_excel(filtered_result)
            print(f"\nâœ… ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {filename}")
        except Exception as e:
            print(f"âš ï¸ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return 0
        
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ ë¶„ì„ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ì§„í–‰ëœ ë¶€ë¶„ê¹Œì§€ ì €ì¥
        if analysis_result and client:
            try:
                print(f"ğŸ’¾ ì§„í–‰ëœ '{KEYWORD_SUBJECT}' ë°ì´í„° ì €ì¥ ì¤‘...")
                # íƒ€ê²Ÿ ìŠ¤í…Œì´ì§€ ì´ë‚´ í•„í„°ë§ (ê¸°ë³¸/ìë™ ëª¨ë“œ ê³µí†µ)
                filtered_result = client.filter_keywords_by_target_stages(analysis_result, TARGET_STAGES)
                
                if filtered_result:
                    filename = client.save_to_excel(filtered_result)
                    print(f"âœ… ì¤‘ë‹¨ëœ '{KEYWORD_SUBJECT}' í™©ê¸ˆí‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {filename}")
                    print(f"ğŸ“Š ì €ì¥ëœ í‚¤ì›Œë“œ ìˆ˜: {len(filtered_result)}ê°œ")
                else:
                    print(f"âš ï¸ '{KEYWORD_SUBJECT}' ì €ì¥í•  í™©ê¸ˆí‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as save_error:
                print(f"âŒ '{KEYWORD_SUBJECT}' ì¤‘ë‹¨ í›„ ì €ì¥ ì‹¤íŒ¨: {save_error}")
        return 1
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì§„í–‰ëœ ë¶€ë¶„ê¹Œì§€ ì €ì¥ ì‹œë„
        if analysis_result and client:
            try:
                print(f"ğŸ’¾ ì˜¤ë¥˜ ë°œìƒ, '{KEYWORD_SUBJECT}' ì§„í–‰ëœ ë°ì´í„° ì €ì¥ ì‹œë„...")
                if MODE == "BASIC":
                    filtered_result = client.filter_keywords_by_stage(analysis_result)
                elif MODE == "AUTO":
                    filtered_result = client.filter_keywords_auto_mode(analysis_result, TARGET_STAGES)
                else:
                    filtered_result = {}
                
                if filtered_result:
                    filename = client.save_to_excel(filtered_result)
                    print(f"âœ… ì˜¤ë¥˜ í›„ '{KEYWORD_SUBJECT}' í™©ê¸ˆí‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {filename}")
                    print(f"ğŸ“Š ì €ì¥ëœ í‚¤ì›Œë“œ ìˆ˜: {len(filtered_result)}ê°œ")
            except Exception as save_error:
                print(f"âŒ '{KEYWORD_SUBJECT}' ì˜¤ë¥˜ í›„ ì €ì¥ ì‹¤íŒ¨: {save_error}")
        return 1


def debug_mode():
    """ë””ë²„ê¹… ëª¨ë“œ - íŠ¹ì • í‚¤ì›Œë“œë“¤ì˜ ê²€ìƒ‰ëŸ‰ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
    log_print("ğŸ”§ ë„¤ì´ë²„ ê´‘ê³  API ê²€ìƒ‰ëŸ‰ ì¡°íšŒ ë””ë²„ê¹… ëª¨ë“œ")
    
    # í…ŒìŠ¤íŠ¸í•  í‚¤ì›Œë“œë“¤ (ìœ ì˜ë¯¸í•´ ë³´ì´ëŠ” í‚¤ì›Œë“œë“¤ë¡œ ì„¤ì •)
    test_keywords = [
        "ê²Œì„",
        "ëª¨ë°”ì¼ê²Œì„", 
        "ì˜¨ë¼ì¸ê²Œì„",
        "ê²Œì„ì¶”ì²œ",
        "RPGê²Œì„",
        "ì•¡ì…˜ê²Œì„",
        "ìŠ¤ë§ˆíŠ¸í°ê²Œì„",
        "PCê²Œì„",
        "ë¬´ë£Œê²Œì„",
        "ì¸ê¸°ê²Œì„"
    ]
    
    try:
        client = NaverAdsClient()
        client.debug_search_volume(test_keywords)
    except Exception as e:
        log_print(f"âŒ ë””ë²„ê¹… ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ë””ë²„ê¹… ëª¨ë“œ ì‹¤í–‰ ê°€ëŠ¥
    if len(sys.argv) > 1 and sys.argv[1].lower() == "debug":
        debug_mode()
    else:
        exit(main())
