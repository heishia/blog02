"""
í…ìŠ¤íŠ¸ í•„í„°ë§ ë° ë‹¨ì–´ ì¶”ì¶œ ëª¨ë“ˆ
- KPF-BERT-NERì„ ì‚¬ìš©í•œ ê²Œì„ ë¸Œëœë“œ/ì œí’ˆëª… ì¶”ì¶œ
- ë¡œì»¬ í•œêµ­ì–´ ë¶ˆìš©ì–´ íŒŒì¼ ì‚¬ìš© (data/stopwords-ko/)
"""

import re
import os
from typing import List, Set, Optional, Tuple, Dict
from collections import Counter
from pathlib import Path

# ============================================================================
# ğŸ“‹ í•„í„°ë§ ì„¤ì •ê°’ë“¤
# ============================================================================

# ğŸ¯ ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì • (ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨!)
KEYWORD_SUBJECT = "ê²Œì„"  # ğŸ” ë¶„ì„í•  í‚¤ì›Œë“œ

# ğŸ¯ ì •í™•ë„ ì„ê³„ê°’ ì„¤ì •
CONFIDENCE_THRESHOLD = 0.8  # ì •í™•ë„ 0.8 ì´ìƒë§Œ ì‹œë“œí‚¤ì›Œë“œë¡œ ë“±ë¡


# ğŸš« ë¡œì»¬ íŒŒì¼ì—ì„œ í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¡œë“œ
def _load_auto_stopwords() -> Set[str]:
    """ë¡œì»¬ stopwords-ko íŒŒì¼ì—ì„œ ë¶ˆìš©ì–´ ë¡œë“œ"""
    auto_stopwords = set()

    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œë“¤
    project_root = Path(__file__).parent.parent.parent  # PPOP_keyword/
    possible_paths = [
        project_root / "data" / "stopwords-ko" / "stopwords-ko.txt",
        project_root / "stopwords-ko" / "stopwords-ko.txt",
        Path("data/stopwords-ko/stopwords-ko.txt"),
        Path("stopwords-ko/stopwords-ko.txt"),
    ]

    for path in possible_paths:
        try:
            if path.exists():
                with open(path, "r", encoding="utf-8") as f:
                    lines = f.read().strip().split("\n")
                    for line in lines:
                        word = line.strip()
                        if word and not word.startswith("#"):  # ì£¼ì„ ì œì™¸
                            auto_stopwords.add(word)

                if auto_stopwords:
                    return auto_stopwords

        except Exception:
            continue

    return set()


# ìë™ ë¶ˆìš©ì–´ ë¡œë“œ
AUTO_STOPWORDS = _load_auto_stopwords()

# ============================================================================
# ğŸ“‹ Subjectë³„ ë¼ë²¨ ê´€ë¦¬ í•¨ìˆ˜
# ============================================================================


def _load_subject_labels_config() -> dict:
    """Subjectë³„ ë¼ë²¨ ì„¤ì •ì„ JSONì—ì„œ ë¡œë“œ"""
    import json

    project_root = Path(__file__).parent.parent.parent
    config_path = project_root / "data" / "subject_labels.json"

    try:
        if config_path.exists():
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception as e:
        print(f"âš ï¸ subject_labels.json ë¡œë“œ ì‹¤íŒ¨: {e}")

    # fallback: ê¸°ë³¸ ì„¤ì • ë°˜í™˜
    return {
        "subject_label_mapping": {},
        "default_target_labels": [
            "B-AFW_OTHER_PRODUCTS",
            "I-AFW_OTHER_PRODUCTS",
            "B-TMIG_GENRE",
            "I-TMIG_GENRE",
            "B-TMI_SERVICE",
            "I-TMI_SERVICE",
            "B-TMI_SW",
            "I-TMI_SW",
            "B-OGG_MEDIA",
            "I-OGG_MEDIA",
        ],
    }


def get_target_labels_for_subject(subject: str) -> set:
    """
    Subjectì— ë§ëŠ” íƒ€ê²Ÿ ë¼ë²¨ë“¤ì„ ë°˜í™˜

    Args:
        subject: í‚¤ì›Œë“œ ì£¼ì œ (ì˜ˆ: "SNS", "GAME")

    Returns:
        set: í•´ë‹¹ subjectì—ì„œ ì‚¬ìš©í•  ë¼ë²¨ë“¤
    """
    config = _load_subject_labels_config()
    subject_mapping = config.get("subject_label_mapping", {})

    if subject in subject_mapping:
        return set(subject_mapping[subject])
    else:
        return set(config.get("default_target_labels", []))


# ============================================================================

# Transformers (KPF-BERT-NER) import
try:
    from transformers import pipeline

    HAS_TRANSFORMERS = True
except ImportError:
    pipeline = None
    HAS_TRANSFORMERS = False

# KPF-BERT-NER ë¼ë²¨ ë§¤í•‘ ì„í¬íŠ¸
try:
    import sys
    from pathlib import Path

    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))

    # ì´ì œ data íŒ¨í‚¤ì§€ì—ì„œ ì§ì ‘ import
    from data.kobert_label import labels, id2label, label2id

    HAS_LABEL_MAPPING = True
except ImportError:
    id2label = {}
    HAS_LABEL_MAPPING = False

# ê¸€ë¡œë²Œ ëª¨ë¸ ìºì‹œ
_NER_MODEL_CACHE = None


def _try_load_kpf_ner() -> Optional[object]:
    """KPF-BERT-NER ëª¨ë¸ ë¡œë“œ"""
    global _NER_MODEL_CACHE

    if _NER_MODEL_CACHE is not None:
        return _NER_MODEL_CACHE

    if HAS_TRANSFORMERS:
        try:
            _NER_MODEL_CACHE = pipeline(
                "ner",
                model="KPF/KPF-bert-ner",
                tokenizer="KPF/KPF-bert-ner",
                aggregation_strategy="max",  # ì„œë¸Œì›Œë“œ ì˜ í•©ì¹˜ê¸°
            )
            print(f"KoBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return _NER_MODEL_CACHE

        except Exception:
            return None
    return None


def _extract_with_kpf_ner(text: str, ner_model) -> List[Tuple[str, float, str]]:
    """KPF-BERT-NERë¡œ ê²Œì„ ë¸Œëœë“œ/ì œí’ˆëª… ì¶”ì¶œ (ì •í™•ë„, ë¼ë²¨ í¬í•¨)"""
    try:
        # ğŸš« ê²€ìƒ‰ í‚¤ì›Œë“œ ê¸°ë°˜ ì œì™¸ ë‹¨ì–´ ìƒì„±
        exclude_words = set()
        if KEYWORD_SUBJECT:
            exclude_words.add(KEYWORD_SUBJECT)
            keyword_parts = KEYWORD_SUBJECT.split()
            exclude_words.update(keyword_parts)

        if len(text) > 500:
            text = text[:500]

        entities = ner_model(text)

        # ğŸ• ì‹œê°„ ê´€ë ¨ ë‹¨ì–´ë“¤ (ì œê±°ìš©)
        time_words = {
            "ì–´ì œ",
            "ì˜¤ëŠ˜",
            "ë‚´ì¼",
            "ì§€ê¸ˆ",
            "ë°©ê¸ˆ",
            "ë‚˜ì¤‘ì—",
            "ì´ë”°ê°€",
            "ìµœê·¼",
            "ì˜ˆì „",
            "ì›”ìš”ì¼",
            "í™”ìš”ì¼",
            "ìˆ˜ìš”ì¼",
            "ëª©ìš”ì¼",
            "ê¸ˆìš”ì¼",
            "í† ìš”ì¼",
            "ì¼ìš”ì¼",
            "ì›”",
            "í™”",
            "ìˆ˜",
            "ëª©",
            "ê¸ˆ",
            "í† ",
            "ì¼",
            "ì•„ì¹¨",
            "ì ì‹¬",
            "ì €ë…",
            "ë°¤",
            "ìƒˆë²½",
            "ì˜¤ì „",
            "ì˜¤í›„",
            "ë‚®",
            "ë°¤ì¤‘",
            "ë´„",
            "ì—¬ë¦„",
            "ê°€ì„",
            "ê²¨ìš¸",
            "ì£¼ë§",
            "í‰ì¼",
            "íœ´ì¼",
            "ì—°íœ´",
            "ë°©í•™",
            "ê³¼ê±°",
            "í˜„ì¬",
            "ë¯¸ë˜",
            "ë‹¹ì‹œ",
            "ê·¸ë•Œ",
            "ì´ë•Œ",
            "ìš”ì¦˜",
            "ê·¼ë˜",
        }

        # ğŸ® Subjectë³„ íƒ€ê²Ÿ ë¼ë²¨ ê°€ì ¸ì˜¤ê¸° (JSONì—ì„œ ë¡œë“œ)
        target_labels = get_target_labels_for_subject(KEYWORD_SUBJECT)

        meaningful_words = []
        for entity in entities:
            entity_group = entity["entity_group"]
            score = entity["score"]
            word = entity["word"].strip()

            # ğŸ”¥ LABEL_ìˆ«ìë¥¼ ì‹¤ì œ ë¼ë²¨ëª…ìœ¼ë¡œ ë³€í™˜
            display_label = entity_group
            actual_label = entity_group

            if entity_group.startswith("LABEL_") and HAS_LABEL_MAPPING:
                try:
                    label_id = int(entity_group.split("_")[1])
                    actual_label = id2label.get(label_id, entity_group)
                    display_label = actual_label
                except:
                    pass

            # ğŸ¯ íƒ€ê²Ÿ ë¼ë²¨ë§Œ í•„í„°ë§
            if actual_label not in target_labels:
                continue

            # ğŸš« ì„œë¸Œì›Œë“œ í† í° ì œê±°
            if word.startswith("##"):
                continue

            # ğŸš« ë„ˆë¬´ ì§§ê±°ë‚˜ ì´ìƒí•œ ë‹¨ì–´ ì œê±°
            if len(word) < 2:
                continue

            # ğŸš« ìˆ«ìê°€ í¬í•¨ëœ ë‹¨ì–´ ì œê±°
            if any(c.isdigit() for c in word):
                continue

            # ğŸš« ì‹œê°„ ê´€ë ¨ ë‹¨ì–´ ì œê±°
            if word in time_words:
                continue

            # ğŸš« íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ë‹¨ì–´ ì œê±°
            if not any(c.isalnum() or c in "ê°€-í£" for c in word):
                continue

            # ğŸš« ì˜ì–´ë‹¨ì–´ ì œê±° (í•œê¸€ì´ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°)
            if not any(c in "ê°€-í£" for c in word):
                continue

            # ğŸš« ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ (ì˜ëª» ì¶”ì¶œëœ ê²ƒ) ì œê±°
            if len(word) > 15:
                continue

            # ğŸš« ë¶ˆìš©ì–´ ì œê±°
            if word in AUTO_STOPWORDS:
                continue

            # ğŸš« ê²€ìƒ‰ í‚¤ì›Œë“œì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë‹¨ì–´ ì œì™¸
            if word in exclude_words:
                continue

            # âœ… ì •í™•ë„ 0.8 ì´ìƒë§Œ ì‹œë“œí‚¤ì›Œë“œë¡œ ë“±ë¡
            if score > CONFIDENCE_THRESHOLD:
                meaningful_words.append((word, score, display_label))  # 3ê°œ ê°’ ë°˜í™˜

        return meaningful_words

    except Exception:
        return []


def extract_game_brands(
    texts: List[str],
    min_len: int = 2,
    max_len: int = 15,
) -> List[Tuple[str, float, str]]:

    # KPF-BERT-NER ëª¨ë¸ ë¡œë“œ
    ner_model = _try_load_kpf_ner()

    if not ner_model:
        print("âŒ KPF-BERT-NER ë¡œë“œ ì‹¤íŒ¨")
        return []

    all_words = []

    # 10% ë‹¨ìœ„ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    total = len(texts)
    for i, text in enumerate(texts):
        if not text:
            continue

        # 10% ë‹¨ìœ„ë¡œë§Œ ì§„í–‰ ìƒí™© í‘œì‹œ
        if total > 100 and (i + 1) % (total // 10) == 0:
            percent = int(((i + 1) / total) * 100)
            print(f"  ğŸ“Š í‚¤ì›Œë“œ ì¶”ì¶œ ì§„í–‰: {percent}%")

        # KPF-NERë¡œ ê²Œì„ ë¸Œëœë“œ/ì œí’ˆëª… ì¶”ì¶œ (ì •í™•ë„, ë¼ë²¨ í¬í•¨)
        words_with_scores = _extract_with_kpf_ner(text, ner_model)
        all_words.extend(words_with_scores)

    print(" KoBERT ëª¨ë¸ë¡œ ì‹œë“œí‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")

    # ìµœì¢… ê¸¸ì´ í•„í„°ë§ë§Œ ì ìš©
    filtered_words = []
    for word, score, label in all_words:
        word = word.strip()
        if min_len <= len(word) <= max_len:
            filtered_words.append((word, score, label))

    return filtered_words


def get_top_confidence_keywords(
    texts: List[str],
    min_len: int = 2,
    max_len: int = 15,
    top_k: int = 100,
) -> List[Dict]:
    """
    ì •í™•ë„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)

    Args:
        texts: ì…ë ¥ í…ìŠ¤íŠ¸ ëª©ë¡
        min_len: ìµœì†Œ ë‹¨ì–´ ê¸¸ì´
        max_len: ìµœëŒ€ ë‹¨ì–´ ê¸¸ì´
        top_k: ìƒìœ„ Kê°œ ë°˜í™˜

    Returns:
        List[Dict]: í‚¤ì›Œë“œ ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    # ê²Œì„ ë¸Œëœë“œ/ì œí’ˆëª… ì¶”ì¶œ (ì •í™•ë„, ë¼ë²¨ í¬í•¨)
    # _extract_with_kpf_nerì—ì„œ ì´ë¯¸ ì˜ì–´, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì í•„í„°ë§ ì™„ë£Œ
    keywords_with_data = extract_game_brands(
        texts=texts,
        min_len=min_len,
        max_len=max_len,
    )

    # í‚¤ì›Œë“œë³„ ìµœê³  ì •í™•ë„ì™€ ë¼ë²¨ ìˆ˜ì§‘
    keyword_best = {}
    for keyword, score, label in keywords_with_data:
        # float32ë¥¼ ì¼ë°˜ floatë¡œ ë³€í™˜
        score = float(score)

        if keyword not in keyword_best:
            keyword_best[keyword] = {"confidence": score, "labels": {label}}
        else:
            # ë” ë†’ì€ ì •í™•ë„ë¡œ ì—…ë°ì´íŠ¸
            if score > keyword_best[keyword]["confidence"]:
                keyword_best[keyword]["confidence"] = score
            # ë¼ë²¨ ì¶”ê°€
            keyword_best[keyword]["labels"].add(label)

    # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    keyword_results = []
    for keyword, data in keyword_best.items():
        keyword_results.append(
            {
                "keyword": keyword,
                "confidence": round(data["confidence"], 3),
                "labels": list(data["labels"]),
            }
        )

    # ì •í™•ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    keyword_results.sort(key=lambda x: x["confidence"], reverse=True)

    return keyword_results[:top_k]


def print_filtering_stats(texts: List[str], final_ranking: List[Tuple[str, int]]):
    """í•„í„°ë§ í†µê³„ ì¶œë ¥"""
    print(
        f"ğŸ“Š ë¶„ì„ ê²°ê³¼: {len(texts)}ê°œ í…ìŠ¤íŠ¸ â†’ {len(final_ranking)}ê°œ ê²Œì„ ë¸Œëœë“œ/ì œí’ˆëª…"
    )


def save_seed_keywords_json(keyword_subject: str, seed_keywords: List[Dict]) -> str:
    """ì‹œë“œí‚¤ì›Œë“œë¥¼ JSONìœ¼ë¡œ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ì— ìƒˆ í‚¤ì›Œë“œë§Œ ì¶”ê°€)"""
    import json
    from datetime import datetime

    # data/keywords/ í´ë”ì— ì €ì¥
    project_root = Path(__file__).parent.parent.parent
    save_dir = project_root / "data" / "keywords"
    save_dir.mkdir(exist_ok=True)

    # ğŸ® ê²Œì„ ê´€ë ¨ ì„œë¸Œì íŠ¸ëŠ” ëª¨ë‘ "ê²Œì„.json"ì— ì €ì¥
    if "ê²Œì„" in keyword_subject:
        filename = "ê²Œì„.json"
    else:
        filename = f"{keyword_subject.replace(' ', '_')}.json"
    
    file_path = save_dir / filename

    # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
    existing_keywords = set()
    existing_data = {"keyword_subject": keyword_subject, "seed_keywords": []}

    if file_path.exists():
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
                # ê¸°ì¡´ í‚¤ì›Œë“œë“¤ì„ setìœ¼ë¡œ ì €ì¥
                existing_keywords = {
                    item["keyword"] for item in existing_data["seed_keywords"]
                }
        except:
            pass

    # ìƒˆë¡œìš´ í‚¤ì›Œë“œë§Œ í•„í„°ë§ (ìˆœì„œëŒ€ë¡œ ì¶”ê°€)
    new_keywords = []
    added_count = 0
    current_order = len(existing_data["seed_keywords"])

    for keyword_info in seed_keywords:
        keyword = keyword_info["keyword"]
        if keyword not in existing_keywords:
            # ì¶”ê°€ ìˆœì„œ ê¸°ë¡
            keyword_info["added_order"] = current_order + added_count + 1
            new_keywords.append(keyword_info)
            added_count += 1

    # ê¸°ì¡´ í‚¤ì›Œë“œì™€ ìƒˆ í‚¤ì›Œë“œ í•©ì¹˜ê¸° (ìˆœì„œëŒ€ë¡œ)
    all_keywords = existing_data["seed_keywords"] + new_keywords

    # ì •í™•ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê¸°ì¡´ ìˆœì„œëŠ” ìœ ì§€)
    # ê¸°ì¡´ í‚¤ì›Œë“œë“¤ì€ ì›ë˜ ìˆœì„œ ìœ ì§€, ìƒˆ í‚¤ì›Œë“œë“¤ë§Œ ì •í™•ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    existing_keywords_list = existing_data["seed_keywords"]
    new_keywords_sorted = sorted(new_keywords, key=lambda x: x.get("confidence", 0), reverse=True)
    
    all_keywords = existing_keywords_list + new_keywords_sorted

    # ì €ì¥í•  ë°ì´í„°
    data = {
        "keyword_subject": keyword_subject,
        "last_updated": datetime.now().isoformat(),
        "total_keywords": len(all_keywords),
        "seed_keywords": all_keywords,
    }

    # JSON íŒŒì¼ ì €ì¥
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ìƒˆë¡œìš´ í‚¤ì›Œë“œ {added_count}ê°œ ì¶”ê°€ë¨ (ì „ì²´: {len(all_keywords)}ê°œ)")
    return str(file_path)


if __name__ == "__main__":
    # ì´ì œ ì „ì—­ ë³€ìˆ˜ KEYWORD_SUBJECT ì‚¬ìš©

    import sys
    from pathlib import Path

    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))

    from common.config import load_merged
    from apps.keyword.crawlers import crawl_all_sources

    # ì„¤ì • ë¡œë“œ (API ìê²©ì¦ëª…ë§Œ)
    try:
        cfg = load_merged("config/base.yaml")
        cred = cfg["credentials"]["naver_openapi"]
    except Exception as e:
        print(f"âš ï¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        cred = {}

    # 1ë‹¨ê³„: í¬ë¡¤ë§ ì‹¤í–‰
    texts = crawl_all_sources(
        keyword=KEYWORD_SUBJECT,
        limit_per_source=500,  # ê° ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ ë¬¸ì„œ ìˆ˜
        cred=cred,
    )

    if not texts:
        print("âŒ ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë„¤ì´ë²„ API ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    # 2ë‹¨ê³„: ì •í™•ë„ ê¸°ì¤€ í‚¤ì›Œë“œ ì¶”ì¶œ
    final_ranking = get_top_confidence_keywords(
        texts=texts,
        min_len=2,  # ìµœì†Œ 2ê¸€ì
        max_len=15,  # ìµœëŒ€ 15ê¸€ì
        top_k=200,  # ìƒìœ„ 100ê°œ
    )

    # 3ë‹¨ê³„: ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: {len(texts)}ê°œ í…ìŠ¤íŠ¸ â†’ {len(final_ranking)}ê°œ ê³ í’ˆì§ˆ í‚¤ì›Œë“œ")

    # ì‹œë“œí‚¤ì›Œë“œ JSON ì €ì¥ (ìƒˆ í‚¤ì›Œë“œë§Œ ì¶”ê°€)
    if final_ranking:
        save_path = save_seed_keywords_json(KEYWORD_SUBJECT, final_ranking)
