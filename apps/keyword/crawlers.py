"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸/ì¹´í˜ OpenAPI í¬ë¡¤ë§ ëª¨ë“ˆ
- ë„¤ì´ë²„ OpenAPI: ë¸”ë¡œê·¸/ì¹´í˜ ìµœì‹  ê¸€ ìˆ˜ì§‘
- ë¸”ë¡œê·¸ ë¬¸ì„œìˆ˜(D) ì¡°íšŒ
"""

import re
import time
import html
from typing import List, Dict
import requests

# ============================================================================
# ğŸ“‹ í¬ë¡¤ë§ ì„¤ì •ê°’ë“¤
# ============================================================================

# ğŸ•·ï¸ í¬ë¡¤ë§ ì„¤ì •
DEFAULT_LIMIT_PER_SOURCE = 100  # ê° ì†ŒìŠ¤ë³„ ê¸°ë³¸ ìˆ˜ì§‘ ë¬¸ì„œ ìˆ˜
API_CALL_DELAY = 0.2  # ë„¤ì´ë²„ API í˜¸ì¶œ ê°„ê²© (ì´ˆ)
MAX_API_DISPLAY = 100  # ë„¤ì´ë²„ API í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìµœëŒ€ ê²°ê³¼ ìˆ˜

# ğŸŒ HTTP ì„¤ì •
HTTP_TIMEOUT = 10  # HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

# ============================================================================


def _strip_html(s: str) -> str:
    """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬"""
    s = html.unescape(s or "")
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def crawl_all_sources(
    keyword: str,
    limit_per_source: int = DEFAULT_LIMIT_PER_SOURCE,
    cred: Dict = None,
    sources: Dict = None,
) -> List[str]:
    """
    ëª¨ë“  ì†ŒìŠ¤ì—ì„œ í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§

    Args:
        keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        limit_per_source: ê° ì†ŒìŠ¤ë³„ ìˆ˜ì§‘í•  ë¬¸ì„œ ìˆ˜
        cred: ë„¤ì´ë²„ API ìê²©ì¦ëª…
        sources: í¬ë¡¤ë§ ì†ŒìŠ¤ ì„¤ì •

    Returns:
        List[str]: ìˆ˜ì§‘ëœ ëª¨ë“  í…ìŠ¤íŠ¸ ëª©ë¡
    """
    if sources is None:
        sources = {"naver_blog": True, "naver_cafe": True}
    if cred is None:
        cred = {}

    print(f"ğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§ ì‹œì‘...")
    all_texts = []

    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§
    if sources.get("naver_blog", False) and cred:
        print("ğŸ“ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì¤‘...")
        blog_texts = crawl_naver_blog(keyword, limit_per_source, cred)
        all_texts.extend(blog_texts)
        print(f"âœ“ ë¸”ë¡œê·¸: {len(blog_texts)}ê°œ ìˆ˜ì§‘")

    # ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§
    if sources.get("naver_cafe", False) and cred:
        print("â˜• ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ ì¤‘...")
        cafe_texts = crawl_naver_cafe(keyword, limit_per_source, cred)
        all_texts.extend(cafe_texts)
        print(f"âœ“ ì¹´í˜: {len(cafe_texts)}ê°œ ìˆ˜ì§‘")
        # DCInside í¬ë¡¤ë§ì€ ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬ë¨
    print(f"ğŸ“Š ì´ {len(all_texts)}ê°œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_texts


def crawl_naver_blog(seed: str, limit: int, cred: Dict) -> List[str]:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ OpenAPIë¡œ ìµœì‹  ê¸€ ìˆ˜ì§‘"""
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": cred["client_id"],
        "X-Naver-Client-Secret": cred["client_secret"],
    }
    params = {"query": seed, "display": min(limit, MAX_API_DISPLAY), "sort": "date"}

    out = []
    fetched = 0

    while fetched < limit:
        params["start"] = fetched + 1
        try:
            r = requests.get(url, headers=headers, params=params, timeout=HTTP_TIMEOUT)
            r.raise_for_status()
            items = r.json().get("items", [])

            for item in items:
                title = item.get("title", "")
                desc = item.get("description", "")
                text = _strip_html(f"{title} {desc}")
                if text:
                    out.append(text)

            got = len(items)
            if got == 0:
                break
            fetched += got
            time.sleep(API_CALL_DELAY)

        except Exception as e:
            print(f"ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì˜¤ë¥˜ ({seed}): {e}")
            break

    return out[:limit]


def crawl_naver_cafe(seed: str, limit: int, cred: Dict) -> List[str]:
    """ë„¤ì´ë²„ ì¹´í˜ OpenAPIë¡œ ìµœì‹  ê¸€ ìˆ˜ì§‘"""
    url = "https://openapi.naver.com/v1/search/cafearticle.json"
    headers = {
        "X-Naver-Client-Id": cred["client_id"],
        "X-Naver-Client-Secret": cred["client_secret"],
    }
    params = {"query": seed, "display": min(limit, MAX_API_DISPLAY), "sort": "date"}

    out = []
    fetched = 0

    while fetched < limit:
        params["start"] = fetched + 1
        try:
            r = requests.get(url, headers=headers, params=params, timeout=HTTP_TIMEOUT)
            r.raise_for_status()
            items = r.json().get("items", [])

            for item in items:
                title = item.get("title", "")
                desc = item.get("description", "")
                text = _strip_html(f"{title} {desc}")
                if text:
                    out.append(text)

            got = len(items)
            if got == 0:
                break
            fetched += got
            time.sleep(API_CALL_DELAY)

        except Exception as e:
            print(f"ì¹´í˜ í¬ë¡¤ë§ ì˜¤ë¥˜ ({seed}): {e}")
            break

    return out[:limit]


def get_naver_blog_total(keyword: str, cred: Dict) -> int:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ APIë¡œ íŠ¹ì • í‚¤ì›Œë“œì˜ ì´ ë¬¸ì„œìˆ˜(D) ì¡°íšŒ"""
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": cred["client_id"],
        "X-Naver-Client-Secret": cred["client_secret"],
    }
    params = {"query": keyword, "display": 1}

    try:
        r = requests.get(url, headers=headers, params=params, timeout=HTTP_TIMEOUT)
        r.raise_for_status()
        return int(r.json().get("total", 0))
    except Exception as e:
        print(f"ë¬¸ì„œìˆ˜ ì¡°íšŒ ì˜¤ë¥˜ ({keyword}): {e}")
        return 0


if __name__ == "__main__":
    # ğŸ¯ í‚¤ì›Œë“œ ì„¤ì • (ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨!)
    SEARCH_KEYWORD = "ëª¨ë°”ì¼ ê²Œì„"  # ğŸ” ê²€ìƒ‰í•  í‚¤ì›Œë“œ

    import sys
    from pathlib import Path

    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))

    from common.config import load_merged

    # ì„¤ì • ë¡œë“œ (API ìê²©ì¦ëª…ë§Œ)
    try:
        cfg = load_merged("config/base.yaml")
        cred = cfg["credentials"]["naver_openapi"]
    except:
        cred = {}

    # í¬ë¡¤ë§ ì‹¤í–‰ (ìœ„ì— ì„¤ì •ëœ ìƒìˆ˜ê°’ë“¤ ì‚¬ìš©)
    texts = crawl_all_sources(SEARCH_KEYWORD, DEFAULT_LIMIT_PER_SOURCE, cred)

    # ê° ì†ŒìŠ¤ë³„ ì œëª© ë¯¸ë¦¬ë³´ê¸° (10ê°œì”©)
    print(f"\nğŸ“‹ '{SEARCH_KEYWORD}' ìˆ˜ì§‘ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")

    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì œëª©
    if cred:
        print("\nğŸ“ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì œëª© (ìµœì‹  10ê°œ):")
        blog_texts = crawl_naver_blog(SEARCH_KEYWORD, 10, cred)
        for i, text in enumerate(blog_texts[:10], 1):
            # ì œëª©ë§Œ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¬¸ì¥ ë˜ëŠ” ì²« 50ì)
            title = text.split(".")[0].split("!")[0].split("?")[0][:50]
            print(f"  {i:2d}. {title}...")

    # ë„¤ì´ë²„ ì¹´í˜ ì œëª©
    if cred:
        print("\nâ˜• ë„¤ì´ë²„ ì¹´í˜ ì œëª© (ìµœì‹  10ê°œ):")
        cafe_texts = crawl_naver_cafe(SEARCH_KEYWORD, 10, cred)
        for i, text in enumerate(cafe_texts[:10], 1):
            title = text.split(".")[0].split("!")[0].split("?")[0][:50]
            print(f"  {i:2d}. {title}...")

    print(f"\nâœ… '{SEARCH_KEYWORD}' í¬ë¡¤ë§ ì™„ë£Œ!")
    print("ğŸ’¡ í‚¤ì›Œë“œ ë¶„ì„ì€ apps/keyword/filter.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
